# app/soup.py
import requests
from bs4 import BeautifulSoup
from typing import Optional
import logging

# Setup basic logging
# Initial logging settings
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Optional: Use a session for connection pooling
# Using sessions to optimize connections
session = requests.Session()
# It's good practice to set a user-agent
# Setting the User-Agent is a good practice so that sites don't block the request.
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
})

def get_page_content(url: str) -> Optional[BeautifulSoup]:
    """Ø¯Ø±ÛŒØ§ÙØª Ùˆ ØªØ¬Ø²ÛŒÙ‡ Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© ØµÙØ­Ù‡ ÙˆØ¨.
    Fetch and parse the content of a web page.
    """
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status() # Raise an exception for bad status codes
        return BeautifulSoup(response.content, 'html.parser')
    except requests.exceptions.RequestException as e:
        logger.error(f"Error fetching {url}: {e}")
        return None
    except Exception as e:
        logger.error(f"Error parsing HTML from {url}: {e}")
        return None

def scrape_main_page_info() -> str:
    """
    Ø§Ø³Ú©Ø±Ù¾ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§.
    Scrape general information from Alibaba.ir's main page.
    """
    url = "https://www.alibaba.ir/"
    soup = get_page_content(url)
    if not soup:
        return "âŒ Ø®Ø·Ø£ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§."

    info_lines = []
    info_lines.append("ğŸ  Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§:")

    # 1. Extract the main services (airline tickets, train tickets, hotels, tours, ...)
    # Extract main services
    try:
        main_services = soup.find_all('a', class_='wrapper-sub-product')
        if main_services:
            services = [service.find('span', class_='text-body-md').get_text(strip=True) for service in main_services if service.find('span', class_='text-body-md')]
            info_lines.append(f"Ø®Ø¯Ù…Ø§Øª Ø§ØµÙ„ÛŒ: {', '.join(services)}")
        else:
            info_lines.append("Ø®Ø¯Ù…Ø§Øª Ø§ØµÙ„ÛŒ: ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    except Exception as e:
        logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®Ø¯Ù…Ø§Øª Ø§ØµÙ„ÛŒ: {e}")
        info_lines.append("Ø®Ø¯Ù…Ø§Øª Ø§ØµÙ„ÛŒ: Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬")

    # 2. Extract footer links (Help, Rules, Contact Us, ...)
    # Extracting footer links
    try:
        footer_links_section = soup.find('div', {'data-v-d38533f6': ''}) # This attribute seems unique.
        if footer_links_section:
            footer_links = footer_links_section.find_all('a', class_='footer-link')
            links_info = [f"{link.get_text(strip=True)} ({link.get('href')})" for link in footer_links]
            info_lines.append(f"Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡: {', '.join(links_info)}")
        else:
            info_lines.append("Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡: ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    except Exception as e:
        logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡: {e}")
        info_lines.append("Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÛŒÙ† ØµÙØ­Ù‡: Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬")

    # 3. Extract the 'Companion of every trip' and 'Companion of all moments of the trip' sections
    # Extract the 'Travel companion for every trip' and 'Travel companion for every moment of the trip' sections'
    try:
        service_benefits = soup.find('ul', {'style': 'grid-template-columns:1fr 1fr 1fr 3fr;gap:16px;'})
        if service_benefits:
            benefit_items = service_benefits.find_all('li', {'data-v-af62246a': ''})
            benefits = []
            for item in benefit_items:
                title_elem = item.find('h3', class_='mt-0 mb-2')
                desc_elem = item.find('span', class_='text-grays-400')
                title = title_elem.get_text(strip=True) if title_elem else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
                desc = desc_elem.get_text(strip=True) if desc_elem else "Ø¨Ø¯ÙˆÙ† ØªÙˆØ¶ÛŒØ­"
                benefits.append(f"{title}: {desc}")
            info_lines.append("Ù…Ø²Ø§ÛŒØ§ÛŒ Ø³ÙØ± Ø¨Ø§ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§:")
            info_lines.extend([f"  - {b}" for b in benefits])
        else:
             info_lines.append("Ù…Ø²Ø§ÛŒØ§ÛŒ Ø³ÙØ± Ø¨Ø§ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§: ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    except Exception as e:
        logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø²Ø§ÛŒØ§ÛŒ Ø³ÙØ±: {e}")
        info_lines.append("Ù…Ø²Ø§ÛŒØ§ÛŒ Ø³ÙØ± Ø¨Ø§ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§: Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬")

    return "\n".join(info_lines)

# --- Additional tools for specific sections ---
# Additional tools for specific sections

def scrape_hotel_page_info() -> str:
    """Ø§Ø³Ú©Ø±Ù¾ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² ØµÙØ­Ù‡ Ù‡ØªÙ„.
    Scrape information from the hotel booking page.
    """
    url = "https://www.alibaba.ir/hotel"
    soup = get_page_content(url)
    if not soup:
        return "âŒ Ø®Ø·Ø£ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ Ù‡ØªÙ„ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§."

    info_lines = []
    info_lines.append("ğŸ¨ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² ØµÙØ­Ù‡ Ø±Ø²Ø±Ùˆ Ù‡ØªÙ„ Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§:")

    # Extract the original text of the page
    # Extract the original text of the page
    try:
        main_paragraph = soup.find('p')
        if main_paragraph:
            info_lines.append(f"ØªÙˆØ¶ÛŒØ­Ø§Øª: {main_paragraph.get_text(strip=True)}")
        else:
            info_lines.append("ØªÙˆØ¶ÛŒØ­Ø§Øª: ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    except Exception as e:
        logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª ØµÙØ­Ù‡ Ù‡ØªÙ„: {e}")
        info_lines.append("ØªÙˆØ¶ÛŒØ­Ø§Øª: Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬")

    return "\n".join(info_lines)
