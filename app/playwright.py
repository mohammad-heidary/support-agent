# app/playwright.py
import asyncio
from playwright.async_api import async_playwright
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Flight Search ---
# Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ø±ÙˆØ§Ø² Ø¯Ø§Ø®Ù„ÛŒ

async def scrape_flight_schedules_async(
    origin: str,
    destination: str,
    date: str
) -> str:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø¨Ù„ÛŒØ· Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø¯Ø± Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§.
    Interactive search for domestic flight tickets on Alibaba.ir.
    """
    logger.info(f"Starting flight schedule search: {origin} -> {destination} on {date}")
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 1024})

            url = "https://www.alibaba.ir/flight-ticket"
            logger.info(f"Navigating to {url}")
            await page.goto(url, wait_until='networkidle')

            # Fill origin
            logger.info("Filling origin field...")
            origin_input_selector = "input[placeholder*='Ù…Ø¨Ø¯Ø§']"
            await page.locator(origin_input_selector).first.fill(origin)
            await page.wait_for_timeout(500)
            try:
                await page.locator("div.suggestion-item").first.click()
            except:
                logger.warning("No suggestion item found for origin.")
                pass

            # Fill destination
            logger.info("Filling destination field...")
            destination_input_selector = "input[placeholder*='Ù…Ù‚ØµØ¯']"
            await page.locator(destination_input_selector).first.fill(destination)
            await page.wait_for_timeout(500)
            try:
                await page.locator("div.suggestion-item").first.click()
            except:
                logger.warning("No suggestion item found for destination.")
                pass

            # Fill date
            logger.info("Filling date field...")
            date_input_selector = "input[placeholder*='ØªØ§Ø±ÛŒØ® Ø±ÙØª']"
            await page.locator(date_input_selector).first.fill(date)

            # Click search button
            logger.info("Clicking search button...")
            search_button_selector = "button:has-text('Ø¬Ø³ØªØ¬Ùˆ')"
            await page.locator(search_button_selector).click()

            # Wait for results
            logger.info("Waiting for results...")
            await page.wait_for_selector("text=Ù¾Ø±ÙˆØ§Ø²", timeout=20000)

            # Extract results
            logger.info("Extracting results...")
            flight_items = await page.locator(".flight-item").all()
            
            results = []
            if not flight_items:
                try:
                    no_result_text = await page.locator("text=Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯").text_content(timeout=2000)
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                except:
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡.")
            else:
                for item in flight_items[:5]:
                    try:
                        airline = await item.locator(".airline-name, .flight-airline").text_content(timeout=1000)
                        departure_time = await item.locator(".departure-time, .flight-departure").text_content(timeout=1000)
                        arrival_time = await item.locator(".arrival-time, .flight-arrival").text_content(timeout=1000)
                        price = await item.locator(".price, .flight-price").text_content(timeout=1000)
                        
                        results.append(
                            f"Ø´Ø±Ú©Øª Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ÛŒÛŒ: {airline.strip()}, "
                            f"Ø²Ù…Ø§Ù† Ù¾Ø±ÙˆØ§Ø²: {departure_time.strip()} - {arrival_time.strip()}, "
                            f"Ù‚ÛŒÙ…Øª: {price.strip()}"
                        )
                    except Exception as e:
                        logger.warning(f"Error extracting data from a flight item: {e}")

            await browser.close()
            logger.info("Browser closed.")

            if results:
                return f"âœˆï¸ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ø±ÙˆØ§Ø² Ø§Ø² {origin} Ø¨Ù‡ {destination} Ø¯Ø± ØªØ§Ø±ÛŒØ® {date}:\n" + "\n".join(results)
            else:
                return f"Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±ÙˆØ§Ø² Ø§Ø² {origin} Ø¨Ù‡ {destination} Ø¯Ø± ØªØ§Ø±ÛŒØ® {date} Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù…."

    except asyncio.TimeoutError:
        logger.error("Timeout occurred during flight schedule scraping.")
        return "â° Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø·ÙˆÙ„ Ú©Ø´ÛŒØ¯. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯."
    except Exception as e:
        logger.error(f"Error during flight schedule scraping: {e}", exc_info=True)
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù„ÛŒØ· Ù‡ÙˆØ§Ù¾ÛŒÙ…Ø§ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

def scrape_flight_schedules(origin: str, destination: str, date: str) -> str:
    """Wrapper Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ async Ø¯Ø± ÛŒÙ‡ event loop."""
    try:
        return asyncio.run(scrape_flight_schedules_async(origin, destination, date))
    except Exception as e:
        logger.error(f"Error in scrape_flight_schedules wrapper: {e}")
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± wrapper Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

# --- Train Search ---
# Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‚Ø·Ø§Ø±

async def scrape_train_schedules_async(
    origin: str,
    destination: str,
    date: str
) -> str:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø²Ù…Ø§Ù†Ø¨Ù†Ø¯ÛŒ Ù‚Ø·Ø§Ø±Ù‡Ø§ Ø¯Ø± Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§.
    Interactive search for train schedules on Alibaba.ir.
    """
    logger.info(f"Starting train schedule search: {origin} -> {destination} on {date}")
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 1024})

            url = "https://www.alibaba.ir/train-ticket"
            logger.info(f"Navigating to {url}")
            await page.goto(url, wait_until='networkidle')

            # Fill origin
            logger.info("Filling origin field...")
            origin_input_selector = "input[placeholder*='Ù…Ø¨Ø¯Ø§']"
            await page.locator(origin_input_selector).first.fill(origin)
            await page.wait_for_timeout(500)
            try:
                await page.locator("div.suggestion-item").first.click()
            except:
                logger.warning("No suggestion item found for origin (train).")
                pass

            # Fill destination
            logger.info("Filling destination field...")
            destination_input_selector = "input[placeholder*='Ù…Ù‚ØµØ¯']"
            await page.locator(destination_input_selector).first.fill(destination)
            await page.wait_for_timeout(500)
            try:
                await page.locator("div.suggestion-item").first.click()
            except:
                logger.warning("No suggestion item found for destination (train).")
                pass

            # Fill date
            logger.info("Filling date field...")
            date_input_selector = "input[placeholder*='ØªØ§Ø±ÛŒØ®']" # Check the actual placeholder
            await page.locator(date_input_selector).first.fill(date)

            # Click search button
            logger.info("Clicking search button...")
            search_button_selector = "button:has-text('Ø¬Ø³ØªØ¬Ùˆ')"
            await page.locator(search_button_selector).click()

            # Wait for results
            logger.info("Waiting for results...")
            await page.wait_for_selector("text=Ù‚Ø·Ø§Ø±", timeout=15000)

            # Extract results
            logger.info("Extracting results...")
            train_items = await page.locator(".train-item").all()
            
            results = []
            if not train_items:
                try:
                    no_result_text = await page.locator("text=Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯").text_content(timeout=2000)
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                except:
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡.")
            else:
                for item in train_items[:5]:
                    try:
                        name = await item.locator(".train-name").text_content(timeout=1000)
                        departure = await item.locator(".departure-time").text_content(timeout=1000)
                        arrival = await item.locator(".arrival-time").text_content(timeout=1000)
                        price = await item.locator(".price").text_content(timeout=1000)
                        results.append(f"Ù‚Ø·Ø§Ø±: {name.strip()}, Ø­Ø±Ú©Øª: {departure.strip()}, Ø±Ø³ÛŒØ¯Ù†: {arrival.strip()}, Ù‚ÛŒÙ…Øª: {price.strip()}")
                    except Exception as e:
                        logger.warning(f"Error extracting data from a train item: {e}")

            await browser.close()
            logger.info("Browser closed.")

            if results:
                return f"ğŸš† Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‚Ø·Ø§Ø± Ø§Ø² {origin} Ø¨Ù‡ {destination} Ø¯Ø± ØªØ§Ø±ÛŒØ® {date}:\n" + "\n".join(results)
            else:
                return f"Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‚Ø·Ø§Ø± Ø§Ø² {origin} Ø¨Ù‡ {destination} Ø¯Ø± ØªØ§Ø±ÛŒØ® {date} Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù…."

    except asyncio.TimeoutError:
        logger.error("Timeout occurred during train schedule scraping.")
        return "â° Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‚Ø·Ø§Ø± Ø·ÙˆÙ„ Ú©Ø´ÛŒØ¯."
    except Exception as e:
        logger.error(f"Error during train schedule scraping: {e}", exc_info=True)
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø²Ù…Ø§Ù†Ø¨Ù†Ø¯ÛŒ Ù‚Ø·Ø§Ø± Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

def scrape_train_schedules(origin: str, destination: str, date: str) -> str:
    """Wrapper Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ async Ø¯Ø± ÛŒÙ‡ event loop."""
    try:
        return asyncio.run(scrape_train_schedules_async(origin, destination, date))
    except Exception as e:
        logger.error(f"Error in scrape_train_schedules wrapper: {e}")
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± wrapper Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

# --- Hotel Search ---
# Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ØªÙ„

async def scrape_hotel_info_async(
    city: str,
    checkin_date: str,
    checkout_date: str
) -> str:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡ØªÙ„ Ø¯Ø± Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§.
    Interactive search for hotel information on Alibaba.ir.
    """
    logger.info(f"Starting hotel search in {city} from {checkin_date} to {checkout_date}")
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 1024})

            url = "https://www.alibaba.ir/hotel"
            logger.info(f"Navigating to {url}")
            await page.goto(url, wait_until='networkidle')

            # Fill destination
            logger.info("Filling destination field...")
            destination_input_selector = "input[placeholder*='Ø´Ù‡Ø± ÛŒØ§ Ù†Ø§Ù… Ù‡ØªÙ„']"
            await page.locator(destination_input_selector).first.fill(city)
            await page.wait_for_timeout(500)
            try:
                await page.locator("div.suggestion-item").first.click()
            except:
                logger.warning("No suggestion item found for hotel destination.")
                pass

            # Fill check-in date
            logger.info("Filling check-in date...")
            checkin_input_selector = "input[placeholder*='ØªØ§Ø±ÛŒØ® ÙˆØ±ÙˆØ¯']"
            await page.locator(checkin_input_selector).first.fill(checkin_date)

            # Fill check-out date
            logger.info("Filling check-out date...")
            checkout_input_selector = "input[placeholder*='ØªØ§Ø±ÛŒØ® Ø®Ø±ÙˆØ¬']"
            await page.locator(checkout_input_selector).first.fill(checkout_date)

            # Click search button
            logger.info("Clicking search button...")
            search_button_selector = "button:has-text('Ø¬Ø³ØªØ¬Ùˆ')"
            await page.locator(search_button_selector).click()

            # Wait for results
            logger.info("Waiting for results...")
            await page.wait_for_selector("text=Ù‡ØªÙ„", timeout=20000)

            # Extract results
            logger.info("Extracting results...")
            hotel_items = await page.locator(".hotel-item, .HotelCard").all()
            
            results = []
            if not hotel_items:
                try:
                    no_result_text = await page.locator("text=Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯").text_content(timeout=2000)
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                except:
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡.")
            else:
                for item in hotel_items[:3]:
                    try:
                        name = await item.locator(".hotel-name, .HotelCard__name").text_content(timeout=1000)
                        try:
                            rating = await item.locator(".hotel-rating, .HotelCard__rating").text_content(timeout=1000)
                        except:
                            rating = "Ø§Ù…ØªÛŒØ§Ø² Ù†Ø¯Ø§Ø±Ø¯"
                        try:
                            price = await item.locator(".price, .HotelCard__price").first.text_content(timeout=1000)
                        except:
                            price = "Ù‚ÛŒÙ…Øª Ù†Ø§Ù…Ø´Ø®Øµ"
                        try:
                            location = await item.locator(".location, .HotelCard__location").text_content(timeout=1000)
                        except:
                            location = ""
                        
                        results.append(
                            f"ğŸ¨ {name.strip()} "
                            f"({rating.strip()}) - "
                            f"{price.strip()} "
                            f"({location.strip()})"
                        )
                    except Exception as e:
                        logger.warning(f"Error extracting data from a hotel item: {e}")

            await browser.close()
            logger.info("Browser closed.")

            if results:
                return f"ğŸ¨ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ØªÙ„ Ø¯Ø± {city} Ø§Ø² {checkin_date} ØªØ§ {checkout_date}:\n" + "\n".join(results)
            else:
                return f"Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‡ØªÙ„ Ø¯Ø± {city} Ø§Ø² {checkin_date} ØªØ§ {checkout_date} Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù…."

    except asyncio.TimeoutError:
        logger.error("Timeout occurred during hotel info scraping.")
        return "â° Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡ØªÙ„ Ø·ÙˆÙ„ Ú©Ø´ÛŒØ¯."
    except Exception as e:
        logger.error(f"Error during hotel info scraping: {e}", exc_info=True)
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡ØªÙ„ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

def scrape_hotel_info(city: str, checkin_date: str, checkout_date: str) -> str:
    """Wrapper Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ async Ø¯Ø± ÛŒÙ‡ event loop."""
    try:
        return asyncio.run(scrape_hotel_info_async(city, checkin_date, checkout_date))
    except Exception as e:
        logger.error(f"Error in scrape_hotel_info wrapper: {e}")
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± wrapper Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

# --- Villa/Accommodation Search ---
# Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÛŒÙ„Ø§/Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡

async def scrape_villa_info_async(
    city: str,
    checkin_date: str,
    checkout_date: str
) -> str:
    """
    Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆÛŒÙ„Ø§/Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡ Ø¯Ø± Ø¹Ù„ÛŒâ€ŒØ¨Ø§Ø¨Ø§.
    Interactive search for villa/accommodation information on Alibaba.ir.
    """
    logger.info(f"Starting villa search in {city} from {checkin_date} to {checkout_date}")
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            await page.set_viewport_size({"width": 1280, "height": 1024})

            url = "https://www.alibaba.ir/accommodation"
            logger.info(f"Navigating to {url}")
            await page.goto(url, wait_until='networkidle')

            # Fill destination
            logger.info("Filling destination field...")
            destination_input_selector = "input[placeholder*='Ø´Ù‡Ø± ÛŒØ§ Ù…Ù†Ø·Ù‚Ù‡']"
            await page.locator(destination_input_selector).first.fill(city)
            await page.wait_for_timeout(500)
            try:
                await page.locator("div.suggestion-item").first.click()
            except:
                logger.warning("No suggestion item found for villa destination.")
                pass

            # Fill check-in date
            logger.info("Filling check-in date...")
            checkin_input_selector = "input[placeholder*='ØªØ§Ø±ÛŒØ® ÙˆØ±ÙˆØ¯']"
            await page.locator(checkin_input_selector).first.fill(checkin_date)

            # Fill check-out date
            logger.info("Filling check-out date...")
            checkout_input_selector = "input[placeholder*='ØªØ§Ø±ÛŒØ® Ø®Ø±ÙˆØ¬']"
            await page.locator(checkout_input_selector).first.fill(checkout_date)

            # Click search button
            logger.info("Clicking search button...")
            search_button_selector = "button:has-text('Ø¬Ø³ØªØ¬Ùˆ')"
            await page.locator(search_button_selector).click()

            # Wait for results
            logger.info("Waiting for results...")
            await page.wait_for_selector("text=Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡", timeout=20000)

            # Extract results
            logger.info("Extracting results...")
            villa_items = await page.locator(".villa-item, .AccommodationCard").all()
            
            results = []
            if not villa_items:
                try:
                    no_result_text = await page.locator("text=Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯").text_content(timeout=2000)
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                except:
                    results.append("Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± ØµÙØ­Ù‡ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡.")
            else:
                for item in villa_items[:3]:
                    try:
                        name = await item.locator(".villa-name, .AccommodationCard__name").text_content(timeout=1000)
                        try:
                            rating = await item.locator(".villa-rating, .AccommodationCard__rating").text_content(timeout=1000)
                        except:
                            rating = "Ø§Ù…ØªÛŒØ§Ø² Ù†Ø¯Ø§Ø±Ø¯"
                        try:
                            price = await item.locator(".price, .AccommodationCard__price").first.text_content(timeout=1000)
                        except:
                            price = "Ù‚ÛŒÙ…Øª Ù†Ø§Ù…Ø´Ø®Øµ"
                        try:
                            location = await item.locator(".location, .AccommodationCard__location").text_content(timeout=1000)
                        except:
                            location = ""
                        
                        results.append(
                            f"ğŸ¡ {name.strip()} "
                            f"({rating.strip()}) - "
                            f"{price.strip()} "
                            f"({location.strip()})"
                        )
                    except Exception as e:
                        logger.warning(f"Error extracting data from a villa item: {e}")

            await browser.close()
            logger.info("Browser closed.")

            if results:
                return f"ğŸ¡ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡ Ø¯Ø± {city} Ø§Ø² {checkin_date} ØªØ§ {checkout_date}:\n" + "\n".join(results)
            else:
                return f"Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡ Ø¯Ø± {city} Ø§Ø² {checkin_date} ØªØ§ {checkout_date} Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù…."

    except asyncio.TimeoutError:
        logger.error("Timeout occurred during villa info scraping.")
        return "â° Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡ Ø·ÙˆÙ„ Ú©Ø´ÛŒØ¯."
    except Exception as e:
        logger.error(f"Error during villa info scraping: {e}", exc_info=True)
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ù‚Ø§Ù…ØªÚ¯Ø§Ù‡ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"

def scrape_villa_info(city: str, checkin_date: str, checkout_date: str) -> str:
    """Wrapper Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ async Ø¯Ø± ÛŒÙ‡ event loop."""
    try:
        return asyncio.run(scrape_villa_info_async(city, checkin_date, checkout_date))
    except Exception as e:
        logger.error(f"Error in scrape_villa_info wrapper: {e}")
        return f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± wrapper Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"
